{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webscraping Player Stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import undetected_chromedriver as uc\n",
    "\n",
    "from collections import OrderedDict\n",
    "import time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to retrieve soup object of webpage\n",
    "\n",
    "Since we will need to do a lot of webscraping to collect our player stat data, we will first create a function that retrieves and parses the HTML content of a given URL, returning a Beautiful Soup object for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soup_object(url):\n",
    "    \"\"\"\n",
    "    Fetches and parses an HTML page from a given URL using BeautifulSoup.\n",
    "\n",
    "    Parameters:\n",
    "    - url (str): The web address of the page to be retrieved.\n",
    "\n",
    "    Returns:\n",
    "    - soup (BeautifulSoup object): Parsed HTML content of the webpage.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define a user-agent string to mimic a real browser request\n",
    "    user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36'\n",
    "\n",
    "    # Set up HTTP headers to include the user-agent\n",
    "    headers = {\n",
    "        'User-Agent': user_agent\n",
    "    }\n",
    "\n",
    "    # Make an HTTP GET request to the specified URL with the custom headers\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    # Parse the HTML content of the response using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Return the BeautifulSoup object for further processing\n",
    "    return soup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to webscrape individual win/loss stats for an individual player\n",
    "\n",
    "We will collect out player stats from the website matchstat.com which contains extensive information on a player's stats across multiple areas such as various breakdowns of their yearly win/loss stats as well as match stats such as serve, return and break point stats.\n",
    "\n",
    "First we will create a function that extracts the below win-loss statistics\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"screenshots/win_loss_tables.png\" width=\"600\">\n",
    "</div>\n",
    "\n",
    "\n",
    "The function will take a specified index name and the Beautiful Soup object as arguments, and return the wins and losses from that index name, i.e. for the above example if the index_name was 'Overall', the function would return 351 and 122 representing the wins and the losses respectively. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_win_loss_stats(soup, index_name):\n",
    "    \"\"\"\n",
    "    Extracts win-loss statistics from an HTML table.\n",
    "\n",
    "    Parameters:\n",
    "    - soup (BeautifulSoup object): Parsed HTML content.\n",
    "    - index_name (str): The text identifier to locate the specific row in the table.\n",
    "\n",
    "    Returns:\n",
    "    - wins (int or NaN): The number of wins extracted from the stats.\n",
    "    - losses (int or NaN): The number of losses extracted from the stats.\n",
    "    \"\"\"\n",
    "\n",
    "    # Iterate through all table rows in the HTML\n",
    "    for row in soup.find_all('tr'):\n",
    "        # Check if the specified index_name is present in the row's text\n",
    "        # and that the row contains exactly three table data (<td>) cells\n",
    "        if (index_name in row.text) & (len(row.find_all('td')) == 3):\n",
    "            # Ensure the first cell exactly matches index_name\n",
    "            if row.find('td').text == index_name:\n",
    "                # Extract the win-loss statistics from the third cell (expected format: \"wins/losses\")\n",
    "                stats = row.find_all('td')[2].text\n",
    "\n",
    "                # Extract the percent from the second column\n",
    "                pct_col = row.find_all('td')[1].text\n",
    "\n",
    "                # Extract and convert wins and losses by splitting on '/'\n",
    "                wins = int(stats.split('/')[0].strip())   # First value (number of wins)\n",
    "                losses = int(stats.split('/')[1].strip()) # Second value (number of losses)\n",
    "\n",
    "                # Extract percentage\n",
    "                try:\n",
    "                    pct = float(pct_col.strip().replace('%', ''))\n",
    "                except:\n",
    "                    pct = 0\n",
    "                \n",
    "                break  # Stop iterating once the correct row is found\n",
    "            else:\n",
    "                # If the row does not exactly match, return 0 for all values\n",
    "                wins = 0\n",
    "                losses = 0\n",
    "                pct = 0\n",
    "        else:\n",
    "            # If no matching row is found, return 0 for all values\n",
    "            wins = 0\n",
    "            losses = 0\n",
    "            pct = 0\n",
    "\n",
    "    # Return the extracted win and loss values\n",
    "    return wins, losses, pct\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to webscrape individual serve/return stats for an individual player\n",
    "\n",
    "Similarly to above, we also need to extract the serve and returns stats which are in a similar format:\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"screenshots/serve_return_tables.png\" width=\"600\">\n",
    "</div>\n",
    "\n",
    "Thereofore, we will build another similar function that takes a specified index name and the Beautiful Soup object as arguments, and return the serves/returns etc and the attempts from that index name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_serve_return_stats(soup, index_name):\n",
    "    \"\"\"\n",
    "    Extracts serve return statistics from an HTML table.\n",
    "\n",
    "    Parameters:\n",
    "    - soup (BeautifulSoup object): Parsed HTML content.\n",
    "    - index_name (str): The text identifier to locate the specific row in the table.\n",
    "\n",
    "    Returns:\n",
    "    - num (int): The first number in the stats (successful returns).\n",
    "    - total (int): The second number in the stats (total attempts).\n",
    "    \"\"\"\n",
    "\n",
    "    # Iterate over all table rows in the parsed HTML\n",
    "    for row in soup.find_all('tr'):\n",
    "        # Check if the row contains the target index_name and has exactly 3 columns\n",
    "        if (index_name in row.text) & (len(row.find_all('td')) == 3):\n",
    "            # Extract the statistics from the third column (assumed format: \"num/total\")\n",
    "            stats = row.find_all('td')[2].text\n",
    "\n",
    "            # Extract the percent from the second column\n",
    "            pct_col = row.find_all('td')[1].text\n",
    "\n",
    "            # Extract and convert numbers from the \"num/total\" format\n",
    "            try:\n",
    "                num = int(stats.split('/')[0].strip())   # First value (successful returns)\n",
    "                total = int(stats.split('/')[1].strip()) # Second value (total attempts)\n",
    "            except: \n",
    "                num = 0\n",
    "                total = 0\n",
    "\n",
    "            # Extract percentage\n",
    "            try:\n",
    "                pct = float(pct_col.strip().replace('%', ''))\n",
    "            except:\n",
    "                pct = 0\n",
    "            \n",
    "            break  # Stop iterating once the correct row is found\n",
    "\n",
    "        else:\n",
    "            num = 0\n",
    "            total = 0\n",
    "            pct = 0\n",
    "\n",
    "    return num, total, pct  # Return extracted values as a tuple\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to webscrape all win/loss stats for an individual player for a specified year\n",
    "\n",
    "Now that we have functions to extract the win and loss stats from each row of the win/loss tables, we now need to build a function that runs this function on all the rows and stores all the values in a dictionary. Therefore, the following function takes the soup object for a certain player's stat page, a dictionary to store the stat values, and a specified year (since we will inevitably want to run this on multiple years), and the returns the dictionary now with new populated fields for each stat and year: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_win_loss_stats_by_year(soup, player_dict, year):\n",
    "    \"\"\"\n",
    "    Extracts and stores win-loss statistics for a given player and year from an HTML table.\n",
    "\n",
    "    Parameters:\n",
    "    - soup (BeautifulSoup object): Parsed HTML content containing the player's match statistics.\n",
    "    - player_dict (dict): A dictionary where extracted statistics will be stored.\n",
    "    - year (int or str): The year for which statistics are being retrieved.\n",
    "\n",
    "    Returns:\n",
    "    - player_dict (dict): Updated dictionary containing win-loss stats for various match categories.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract overall win-loss statistics\n",
    "    player_dict['wins_' + str(year)] = get_win_loss_stats(soup, 'Overall')[0]\n",
    "    player_dict['losses_' + str(year)] = get_win_loss_stats(soup, 'Overall')[1]\n",
    "    player_dict['win_pct_' + str(year)] = get_win_loss_stats(soup, 'Overall')[2]\n",
    "\n",
    "    # Extract win-loss stats by surface type\n",
    "    player_dict['hard_wins_' + str(year)] = get_win_loss_stats(soup, 'Hard')[0]\n",
    "    player_dict['hard_losses_' + str(year)] = get_win_loss_stats(soup, 'Hard')[1]\n",
    "    player_dict['hard_pct_' + str(year)] = get_win_loss_stats(soup, 'Hard')[2]\n",
    "    player_dict['clay_wins_' + str(year)] = get_win_loss_stats(soup, 'Clay')[0]\n",
    "    player_dict['clay_losses_' + str(year)] = get_win_loss_stats(soup, 'Clay')[1]   \n",
    "    player_dict['clay_pct_' + str(year)] = get_win_loss_stats(soup, 'Clay')[2]\n",
    "    player_dict['grass_wins_' + str(year)] = get_win_loss_stats(soup, 'Grass')[0]\n",
    "    player_dict['grass_losses_' + str(year)] = get_win_loss_stats(soup, 'Grass')[1]\n",
    "    player_dict['grass_pct_' + str(year)] = get_win_loss_stats(soup, 'Grass')[2]\n",
    "\n",
    "    # Extract win-loss stats by tournament stage\n",
    "    player_dict['final_wins_' + str(year)] = get_win_loss_stats(soup, 'Final')[0]\n",
    "    player_dict['final_losses_' + str(year)] = get_win_loss_stats(soup, 'Final')[1]    \n",
    "    player_dict['final_pct_' + str(year)] = get_win_loss_stats(soup, 'Final')[2]\n",
    "    player_dict['semifinal_wins_' + str(year)] = get_win_loss_stats(soup, 'Semi-final')[0]\n",
    "    player_dict['semifinal_losses_' + str(year)] = get_win_loss_stats(soup, 'Semi-final')[1]\n",
    "    player_dict['semifinal_pct_' + str(year)] = get_win_loss_stats(soup, 'Semi-final')[2]\n",
    "    player_dict['quarterfinal_wins_' + str(year)] = get_win_loss_stats(soup, 'Quarter-final')[0]\n",
    "    player_dict['quarterfinal_losses_' + str(year)] = get_win_loss_stats(soup, 'Quarter-final')[1]\n",
    "    player_dict['quarterfinal_pct_' + str(year)] = get_win_loss_stats(soup, 'Quarter-final')[2]\n",
    "    player_dict['round4_wins_' + str(year)] = get_win_loss_stats(soup, '4th Round')[0]\n",
    "    player_dict['round4_losses_' + str(year)] = get_win_loss_stats(soup, '4th Round')[1]\n",
    "    player_dict['round4_pct_' + str(year)] = get_win_loss_stats(soup, '4th Round')[2]\n",
    "    player_dict['round3_wins_' + str(year)] = get_win_loss_stats(soup, '3rd Round')[0]\n",
    "    player_dict['round3_losses_' + str(year)] = get_win_loss_stats(soup, '3rd Round')[1]\n",
    "    player_dict['round3_pct_' + str(year)] = get_win_loss_stats(soup, '3rd Round')[2]\n",
    "    player_dict['round2_wins_' + str(year)] = get_win_loss_stats(soup, '2nd Round')[0]\n",
    "    player_dict['round2_losses_' + str(year)] = get_win_loss_stats(soup, '2nd Round')[1]\n",
    "    player_dict['round2_pct_' + str(year)] = get_win_loss_stats(soup, '2nd Round')[2]\n",
    "    player_dict['round1_wins_' + str(year)] = get_win_loss_stats(soup, '1st Round')[0]\n",
    "    player_dict['round1_losses_' + str(year)] = get_win_loss_stats(soup, '1st Round')[1]\n",
    "    player_dict['round1_pct_' + str(year)] = get_win_loss_stats(soup, '1st Round')[2]\n",
    "\n",
    "    # Extract win-loss stats by tournament type\n",
    "    player_dict['grand_slam_wins_' + str(year)] = get_win_loss_stats(soup, 'Grand Slam')[0]\n",
    "    player_dict['grand_slam_losses_' + str(year)] = get_win_loss_stats(soup, 'Grand Slam')[1]\n",
    "    player_dict['grand_slam_pct_' + str(year)] = get_win_loss_stats(soup, 'Grand Slam')[2]\n",
    "    player_dict['masters_wins_' + str(year)] = get_win_loss_stats(soup, 'Masters')[0]\n",
    "    player_dict['masters_losses_' + str(year)] = get_win_loss_stats(soup, 'Masters')[1]\n",
    "    player_dict['masters_pct_' + str(year)] = get_win_loss_stats(soup, 'Masters')[2]\n",
    "    player_dict['main_tour_wins_' + str(year)] = get_win_loss_stats(soup, 'Main Tour')[0]\n",
    "    player_dict['main_tour_losses_' + str(year)] = get_win_loss_stats(soup, 'Main Tour')[1]\n",
    "    player_dict['main_tour_pct_' + str(year)] = get_win_loss_stats(soup, 'Main Tour')[2]\n",
    "    player_dict['cup_wins_' + str(year)] = get_win_loss_stats(soup, 'Cups')[0]\n",
    "    player_dict['cup_losses_' + str(year)] = get_win_loss_stats(soup, 'Cups')[1]\n",
    "    player_dict['cup_pct_' + str(year)] = get_win_loss_stats(soup, 'Cups')[2]\n",
    "\n",
    "    # Extract win-loss stats against top-ranked players\n",
    "    player_dict['vsTop5_wins_' + str(year)] = get_win_loss_stats(soup, 'Vs Top.5')[0]\n",
    "    player_dict['vsTop5_losses_' + str(year)] = get_win_loss_stats(soup, 'Vs Top.5')[1]\n",
    "    player_dict['vsTop5_pct_' + str(year)] = get_win_loss_stats(soup, 'Vs Top.5')[2]\n",
    "    player_dict['vsTop10_wins_' + str(year)] = get_win_loss_stats(soup, 'Vs Top.10')[0]\n",
    "    player_dict['vsTop10_losses_' + str(year)] = get_win_loss_stats(soup, 'Vs Top.10')[1]\n",
    "    player_dict['vsTop10_pct_' + str(year)] = get_win_loss_stats(soup, 'Vs Top.10')[2]\n",
    "    player_dict['vsTop20_wins_' + str(year)] = get_win_loss_stats(soup, 'Vs Top.20')[0]\n",
    "    player_dict['vsTop20_losses_' + str(year)] = get_win_loss_stats(soup, 'Vs Top.20')[1]\n",
    "    player_dict['vsTop20_pct_' + str(year)] = get_win_loss_stats(soup, 'Vs Top.20')[2]\n",
    "    player_dict['vsTop50_wins_' + str(year)] = get_win_loss_stats(soup, 'Vs Top.50')[0]\n",
    "    player_dict['vsTop50_losses_' + str(year)] = get_win_loss_stats(soup, 'Vs Top.50')[1]\n",
    "    player_dict['vsTop50_pct_' + str(year)] = get_win_loss_stats(soup, 'Vs Top.50')[2]\n",
    "    player_dict['vsTop100_wins_' + str(year)] = get_win_loss_stats(soup, 'Vs Top.100')[0]\n",
    "    player_dict['vsTop100_losses_' + str(year)] = get_win_loss_stats(soup, 'Vs Top.100')[1]\n",
    "    player_dict['vsTop100_pct_' + str(year)] = get_win_loss_stats(soup, 'Vs Top.100')[2]\n",
    "\n",
    "    return player_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to webscrape all serve/return stats for an individual player for a specified year\n",
    "\n",
    "We can then build a similar function for the serve and return stats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_serve_return_stats_by_year(soup, player_dict, year):\n",
    "    \"\"\"\n",
    "    Extracts and stores serve and return statistics for a given player and year from an HTML table.\n",
    "\n",
    "    Parameters:\n",
    "    - soup (BeautifulSoup object): Parsed HTML content containing the player's match statistics.\n",
    "    - player_dict (dict): A dictionary where extracted statistics will be stored.\n",
    "    - year (int or str): The year for which statistics are being retrieved.\n",
    "\n",
    "    Returns:\n",
    "    - player_dict (dict): Updated dictionary containing serve and return stats for the given year.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract serve-related statistics\n",
    "    player_dict['total_games_' + str(year)] = get_serve_return_stats(soup, 'Aces per Game')[1]\n",
    "    player_dict['aces_' + str(year)] = get_serve_return_stats(soup, 'Aces per Game')[0]\n",
    "    player_dict['aces_pct_' + str(year)] = get_serve_return_stats(soup, 'Aces per Game')[2]    \n",
    "    player_dict['opp_aces_' + str(year)] = get_serve_return_stats(soup, 'Opponent Aces / Game')[0]\n",
    "    player_dict['opp_aces_pct_' + str(year)] = get_serve_return_stats(soup, 'Opponent Aces / Game')[2]\n",
    "    player_dict['DFs_' + str(year)] = get_serve_return_stats(soup, 'Double Faults Per Game')[0]\n",
    "    player_dict['DFs_pct_' + str(year)] = get_serve_return_stats(soup, 'Double Faults Per Game')[2]\n",
    "    player_dict['opp_DFs_' + str(year)] = get_serve_return_stats(soup, 'Opponent Double Faults / Game')[0]\n",
    "    player_dict['opp_DFs_pct_' + str(year)] = get_serve_return_stats(soup, 'Opponent Double Faults / Game')[2]\n",
    "\n",
    "    # Extract 1st and 2nd serve percentage and win percentage\n",
    "    player_dict['1st_serve_' + str(year)] = get_serve_return_stats(soup, '1st Serve %')[0]\n",
    "    player_dict['1st_serve_total_' + str(year)] = get_serve_return_stats(soup, '1st Serve %')[1]\n",
    "    player_dict['1st_serve_pct_' + str(year)] = get_serve_return_stats(soup, '1st Serve %')[2]\n",
    "    player_dict['1st_serve_win_' + str(year)] = get_serve_return_stats(soup, '1st Serve Win%')[0]\n",
    "    player_dict['1st_serve_win_total_' + str(year)] = get_serve_return_stats(soup, '1st Serve Win%')[1]\n",
    "    player_dict['1st_serve_win_pct_' + str(year)] = get_serve_return_stats(soup, '1st Serve Win%')[2]\n",
    "    player_dict['2nd_serve_win_' + str(year)] = get_serve_return_stats(soup, '2nd Serve Win%')[0]\n",
    "    player_dict['2nd_serve_win_total_' + str(year)] = get_serve_return_stats(soup, '2nd Serve Win%')[1]\n",
    "    player_dict['2nd_serve_win_pct_' + str(year)] = get_serve_return_stats(soup, '2nd Serve Win%')[2]\n",
    "    player_dict['serve_pts_win_' + str(year)] = get_serve_return_stats(soup, 'Serve Points Win%')[0]\n",
    "    player_dict['serve_pts_win_total_' + str(year)] = get_serve_return_stats(soup, 'Serve Points Win%')[1]\n",
    "    player_dict['serve_pts_win_pct_' + str(year)] = get_serve_return_stats(soup, 'Serve Points Win%')[2]\n",
    "\n",
    "    # Extract opponent serve statistics\n",
    "    player_dict['opp_1st_serve_' + str(year)] = get_serve_return_stats(soup, 'Opponent 1st Serve %')[0]\n",
    "    player_dict['opp_1st_serve_total_' + str(year)] = get_serve_return_stats(soup, 'Opponent 1st Serve %')[1]\n",
    "    player_dict['opp_1st_serve_pct_' + str(year)] = get_serve_return_stats(soup, 'Opponent 1st Serve %')[2]\n",
    "\n",
    "    # Extract return statistics\n",
    "    player_dict['1st_rtn_win_' + str(year)] = get_serve_return_stats(soup, '1st Return Win %')[0]\n",
    "    player_dict['1st_rtn_win_total_' + str(year)] = get_serve_return_stats(soup, '1st Return Win %')[1]\n",
    "    player_dict['1st_rtn_win_pct_' + str(year)] = get_serve_return_stats(soup, '1st Return Win %')[2]\n",
    "    player_dict['2nd_rtn_win_' + str(year)] = get_serve_return_stats(soup, '2nd Return Win %')[0]\n",
    "    player_dict['2nd_rtn_win_total_' + str(year)] = get_serve_return_stats(soup, '2nd Return Win %')[1]\n",
    "    player_dict['2nd_rtn_win_pct_' + str(year)] = get_serve_return_stats(soup, '2nd Return Win %')[2]\n",
    "    player_dict['rtn_pts_win_' + str(year)] = get_serve_return_stats(soup, 'Return Points Win%')[0]\n",
    "    player_dict['rtn_pts_win_total_' + str(year)] = get_serve_return_stats(soup, 'Return Points Win%')[1]\n",
    "    player_dict['rtn_pts_win_pct_' + str(year)] = get_serve_return_stats(soup, 'Return Points Win%')[2]\n",
    "\n",
    "    # Extract break point statistics\n",
    "    player_dict['bps_saved_' + str(year)] = get_serve_return_stats(soup, 'Break Points Saved / Game')[0]  \n",
    "    player_dict['bps_saved_pct_' + str(year)] = get_serve_return_stats(soup, 'Break Points Saved / Game')[2]       \n",
    "    player_dict['bps_faced_' + str(year)] = get_serve_return_stats(soup, 'Break Points Faced / Game')[0]\n",
    "    player_dict['bps_faced_pct_' + str(year)] = get_serve_return_stats(soup, 'Break Points Faced / Game')[2]\n",
    "    player_dict['bp_save_' + str(year)] = get_serve_return_stats(soup, 'Break Points Save %')[0]\n",
    "    player_dict['bp_save_pct_' + str(year)] = get_serve_return_stats(soup, 'Break Points Save %')[2]\n",
    "    player_dict['bp_save_total_' + str(year)] = get_serve_return_stats(soup, 'Break Points Save %')[1]\n",
    "    player_dict['bp_save_total_pct_' + str(year)] = get_serve_return_stats(soup, 'Break Points Save %')[2]\n",
    "\n",
    "    # Extract service hold and opponent break statistics\n",
    "    player_dict['service_hold_' + str(year)] = get_serve_return_stats(soup, 'Service Hold %')[0]    \n",
    "    player_dict['service_hold_pct_' + str(year)] = get_serve_return_stats(soup, 'Service Hold %')[2]  \n",
    "    player_dict['bps_won_' + str(year)] = get_serve_return_stats(soup, 'Break Points Won / Game')[0]   \n",
    "    player_dict['bps_won_pct_' + str(year)] = get_serve_return_stats(soup, 'Break Points Won / Game')[2]       \n",
    "    player_dict['bps_opps_' + str(year)] = get_serve_return_stats(soup, 'Opponent Break Points / Game')[0] \n",
    "    player_dict['bps_opps_pct_' + str(year)] = get_serve_return_stats(soup, 'Opponent Break Points / Game')[2]         \n",
    "    player_dict['bps_won_' + str(year)] = get_serve_return_stats(soup, 'Break Points Won %')[0]\n",
    "    player_dict['bps_won_pct_' + str(year)] = get_serve_return_stats(soup, 'Break Points Won %')[2]\n",
    "    player_dict['bps_won_total_' + str(year)] = get_serve_return_stats(soup, 'Break Points Won %')[1] \n",
    "    player_dict['bps_won_total_pct_' + str(year)] = get_serve_return_stats(soup, 'Break Points Won %')[2]       \n",
    "    player_dict['opp_hold_' + str(year)] = get_serve_return_stats(soup, 'Opponent Hold %')[0]\n",
    "    player_dict['opp_hold_pct_' + str(year)] = get_serve_return_stats(soup, 'Opponent Hold %')[2]\n",
    "\n",
    "    return player_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to webscrape all win/loss stats for an individual player for all years\n",
    "\n",
    "Now we need to use Selenium WebDriver to manually change the year options on the page so that we can scrape the stats for all years for each player. This is done by gettting the xpath of the dropdown menu below, and cycling through and clicking the options, starting at the second to avoid 'Career' and clicking the next 24 options to cover all years back to 2001 to ensure we cover all our match data. Since some retired players will have their most recent year's data as before the current year, we will include a break clause once we reach 2000. \n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"screenshots/win_loss_dropdown.png\" width=\"500\">\n",
    "</div>\n",
    "\n",
    "Once we have loaded a page, we wait a second for the page to load, and then get the soup object of the page and update the player dictionary with the stat fields for that year. The function then finally returns the fully populated dictionary with all win/loss stats for evey year for that player "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_win_loss_scraping(driver, player_dict):\n",
    "    \"\"\"\n",
    "    Scrapes win-loss statistics for a player from a webpage using Selenium.\n",
    "\n",
    "    Parameters:\n",
    "    - driver (WebDriver): Selenium WebDriver instance controlling the browser.\n",
    "    - player_dict (dict): A dictionary where extracted statistics will be stored.\n",
    "\n",
    "    Returns:\n",
    "    - player_dict (dict): Updated dictionary containing win-loss stats for multiple years.\n",
    "    \"\"\"\n",
    "\n",
    "    dropdown_xpath = '/html/body/app-root/div/app-profile/div/div/div[1]/div[2]/app-profile-performance-breakdown/div[1]/div/h2/app-profile-performance-year-picker/div/a'\n",
    "\n",
    "    previous_stats = None\n",
    "\n",
    "    for index in range(2, 26):\n",
    "        delay = 0.5\n",
    "        max_delay = 7.5\n",
    "        success = False\n",
    "\n",
    "        while delay <= max_delay:\n",
    "            try:\n",
    "                wait = WebDriverWait(driver, delay)\n",
    "                dropdown_link = wait.until(EC.element_to_be_clickable((By.XPATH, dropdown_xpath)))\n",
    "                driver.execute_script(\"arguments[0].click();\", dropdown_link)\n",
    "\n",
    "                year_xpath = f'/html/body/app-root/div/app-profile/div/div/div[1]/div[2]/app-profile-performance-breakdown/div[1]/div/h2/app-profile-performance-year-picker/div/div/div/div/ul/li[{index}]/a'\n",
    "                year_option = driver.find_element(By.XPATH, year_xpath)\n",
    "\n",
    "                success = True\n",
    "                break  # Element found and clickable, proceed\n",
    "            except:\n",
    "                delay += 1.0  # Increase delay and try again\n",
    "\n",
    "        if not success:\n",
    "            continue  # Skip to next index if still unsuccessful after max_delay\n",
    "\n",
    "\n",
    "        year = year_option.text\n",
    "\n",
    "        if int(year) <= 2003:\n",
    "            break\n",
    "\n",
    "        driver.execute_script(\"arguments[0].scrollIntoView(true);\", year_option)\n",
    "        driver.execute_script(\"arguments[0].click();\", year_option)\n",
    "\n",
    "        delay = 0.5\n",
    "        max_delay = 7.5\n",
    "        stats_changed = False\n",
    "\n",
    "        while delay <= max_delay:\n",
    "            time.sleep(delay)\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "            current_stats = {}\n",
    "            get_win_loss_stats_by_year(soup, current_stats, year)\n",
    "\n",
    "            if current_stats != previous_stats:\n",
    "                player_dict.update(current_stats)\n",
    "                previous_stats = current_stats\n",
    "                stats_changed = True\n",
    "                break\n",
    "            else:\n",
    "                delay += 1.0\n",
    "\n",
    "        if not stats_changed:\n",
    "            player_dict.update(current_stats)\n",
    "            previous_stats = current_stats\n",
    "\n",
    "    return player_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to webscrape all serve/return stats for an individual player for all years\n",
    "\n",
    "Similarly to above, we can create a function that uses Selenium to cycle through the years on the dropdown menu in the serve and returns section, subsquently use our previous function to scrape the data from here, and finally return a dictionary containing all the stats information for each year\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"screenshots/serve_return_dropdown.png\" width=\"500\">\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_serve_return_scraping(driver, player_dict):\n",
    "    \"\"\"\n",
    "    Scrapes serve and return statistics for a player from a webpage using Selenium.\n",
    "\n",
    "    Parameters:\n",
    "    - driver (WebDriver): Selenium WebDriver instance controlling the browser.\n",
    "    - player_dict (dict): A dictionary where extracted statistics will be stored.\n",
    "\n",
    "    Returns:\n",
    "    - player_dict (dict): Updated dictionary containing serve and return stats for multiple years.\n",
    "    \"\"\"\n",
    "\n",
    "    parent_div = driver.find_element(By.XPATH, '//div[@class=\"col col-md-2 p-1 p-md-2\"]')\n",
    "    dropdown = Select(parent_div.find_element(By.CLASS_NAME, 'bg-white'))\n",
    "    options = dropdown.options\n",
    "\n",
    "    previous_values = None\n",
    "\n",
    "    for option in options:\n",
    "        year = option.text\n",
    "\n",
    "        if int(year) == 2025:\n",
    "            continue\n",
    "\n",
    "        if int(year) <= 2003:\n",
    "            break\n",
    "\n",
    "        dropdown.select_by_visible_text(year)\n",
    "\n",
    "        delay = 0.5\n",
    "        max_delay = 7.5\n",
    "        stats_changed = False\n",
    "\n",
    "        while delay <= max_delay:\n",
    "            time.sleep(delay)\n",
    "\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "            current_stats = {}\n",
    "            get_serve_return_stats_by_year(soup, current_stats, year)\n",
    "\n",
    "            current_values = list(current_stats.values())\n",
    "\n",
    "            if current_values != previous_values:\n",
    "                player_dict.update(current_stats)\n",
    "                previous_values = current_values\n",
    "                stats_changed = True\n",
    "                break\n",
    "            else:\n",
    "                delay += 1.0  # Increase delay if stats haven't changed\n",
    "\n",
    "        if not stats_changed:\n",
    "            # Stats remained the same even after max_delay, so keep them\n",
    "            player_dict.update(current_stats)\n",
    "            previous_values = list(current_stats.values())\n",
    "\n",
    "    return player_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to webscrape all stats for an individual player for all years\n",
    "\n",
    "The following function then ties all the above together by taking a player's url, starting the Chrome webdriver, creating a soup object to scrape and store the player's name, then running the above functions to run through the year options on the win/loss and serve/return tables to scrape and store all stat data for each year. \n",
    "\n",
    "Given that the website may ocassionally deny requests, or the pages may take longer than expected to load, the function will wait three minutes should it run into an error with the webscraping. If this error persists three times then it will exit the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_player_data(url):\n",
    "    \"\"\"\n",
    "    Scrapes player performance data from a given URL using Selenium and BeautifulSoup.\n",
    "\n",
    "    Parameters:\n",
    "    - url (str): The webpage URL containing the player's data.\n",
    "\n",
    "    Returns:\n",
    "    - player_dict (dict): A dictionary containing the player's name, win/loss stats, and serve/return stats.\n",
    "    - None: Returns None if all retries fail due to errors.\n",
    "    \"\"\"\n",
    "\n",
    "    max_retries = 5  # Maximum number of retry attempts in case of errors\n",
    "    attempt = 0  # Track the number of attempts\n",
    "\n",
    "    while attempt < max_retries:\n",
    "        # Initialise an empty dictionary to store player data\n",
    "        player_dict = {}\n",
    "\n",
    "        # Set Chrome options to disable notifications\n",
    "        chrome_options = webdriver.ChromeOptions()\n",
    "        chrome_options.add_argument(\"--disable-notifications\")\n",
    "\n",
    "        # Start an undetected Chrome WebDriver instance with the specified options\n",
    "        driver = uc.Chrome(options=chrome_options)\n",
    "\n",
    "        try:\n",
    "            # Navigate to the given URL\n",
    "            driver.get(url)\n",
    "\n",
    "            # Wait briefly to allow the page to load\n",
    "            time.sleep(2)\n",
    "\n",
    "            # Extract the webpage content using BeautifulSoup\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "            # Handle cases where the page does not load properly due to a gateway timeout or connection issue\n",
    "            if ('505 Gateway Timeout' in soup.text) or (\"This site can't be reached\" in soup.text) or ('Your connection was interrupted' in soup.text):\n",
    "                time.sleep(120)  # Wait 2 minutes before retrying\n",
    "                driver.get(url)  # Reload the page\n",
    "                soup = BeautifulSoup(driver.page_source, 'html.parser')  # Extract content again\n",
    "\n",
    "            # Extract and store the player's name\n",
    "            player_dict['name'] = soup.find('h4').text\n",
    "\n",
    "            # Scrape win-loss statistics and update the dictionary\n",
    "            perform_win_loss_scraping(driver, player_dict)\n",
    "\n",
    "            # Scrape serve and return statistics and update the dictionary\n",
    "            perform_serve_return_scraping(driver, player_dict)\n",
    "\n",
    "            # Close the WebDriver after successful data extraction\n",
    "            driver.quit()\n",
    "\n",
    "            return player_dict  # Return the collected player data on success\n",
    "\n",
    "        except Exception as e:\n",
    "            attempt += 1\n",
    "            driver.quit()\n",
    "            # Wait time logic: 5 seconds for the first three attempts, 120 seconds for the next two\n",
    "            if attempt < 3:\n",
    "                time.sleep(5)\n",
    "            else:\n",
    "                time.sleep(120)\n",
    "\n",
    "    # If all retry attempts fail, print a message and return None\n",
    "    print(f'Maximum retries reached for {url}. Exiting function.')\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to webscrape all player urls\n",
    "\n",
    "The next step is to get the above function to run on all players, and to do this we need to get a list of all player urls. We can gather this by using BeautifulSoup to scrape the urls from the links contained within the player names from the tennis rankings page from matchstat.com:\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"screenshots/player_rankings.png\" width=\"500\">\n",
    "</div>\n",
    "\n",
    "Since some players would have retired and not be in the latest rankings, or some players may have dropped out the current rankings, we will have to cycle through previous rankings by using a webdriver to select the options from the dropdown menu similar to what we did above. However, in this dropdown menu we have weekly rankings, so to save time we will only select the last week of each year (stopping at 2001).\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"screenshots/rankings_dropdown.png\" width=\"500\">\n",
    "</div>\n",
    "\n",
    "Once we load each yearly rankings page, we will have to use the webdriver to then repeatedly click 'View More' until this option is no longer available to ensure that the whole ranking list is available to scrape Ih the year is 2011 or lower we will only click 'View More' four times as these years only have 500 players available and clicking it an additional time causes the list to disappear.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"screenshots/rankings_view_more.png\" width=\"500\">\n",
    "</div>\n",
    "\n",
    "Once the rankings page is loaded, we will webscrape the urls by finding the 'href' links contained within the players names, and add these to a list if the url does not already exist in the list. The function then finally saves the list as a csv file and then returns this list of player urls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_player_urls(url):\n",
    "    \"\"\"\n",
    "    Scrapes player profile URLs from a rankings webpage using Selenium.\n",
    "\n",
    "    Parameters:\n",
    "    - url (str): The webpage URL containing player rankings.\n",
    "\n",
    "    Returns:\n",
    "    - player_urls (list): A list of extracted player profile URLs.\n",
    "    \"\"\"\n",
    "\n",
    "    player_urls = []  # List to store player profile URLs\n",
    "\n",
    "    # Set Chrome options to disable notifications\n",
    "    chrome_options = uc.ChromeOptions()\n",
    "    chrome_options.add_argument(\"--disable-notifications\")\n",
    "\n",
    "    # Start an undetected Chrome WebDriver instance with the specified options\n",
    "    driver = uc.Chrome(options=chrome_options)\n",
    "    driver.get(url)\n",
    "\n",
    "    # Locate the rankings dropdown element and initialize a Select object\n",
    "    dropdown_element = WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.XPATH, '/html/body/app-root/div/app-rankings/div/div/div[1]/div[5]/form/div[1]/app-selector/select'))\n",
    "    )\n",
    "    dropdown = Select(dropdown_element)\n",
    "\n",
    "    # Retrieve all available options from the dropdown\n",
    "    options = dropdown.options\n",
    "\n",
    "    # Dictionary to store the first occurrence of each year in the dropdown\n",
    "    first_of_each_year = OrderedDict()\n",
    "    first_option_texts = []  # Store option TEXT instead of elements\n",
    "\n",
    "    # Iterate through dropdown options and extract unique years\n",
    "    for option in options:\n",
    "        date_text = option.text  # Extract the option text (e.g., \"March 2024\")\n",
    "        year = date_text.split()[-1]  # Extract the year from the text\n",
    "\n",
    "        # Store the first occurrence of each year\n",
    "        if year not in first_of_each_year:\n",
    "            first_of_each_year[year] = date_text\n",
    "            first_option_texts.append(date_text)\n",
    "\n",
    "        # Stop collecting data if the year is 2003 (since the rankings are missing between 2003 and 1989)\n",
    "        if int(year) == 3:\n",
    "            break\n",
    "    first_option_texts.reverse()\n",
    "    \n",
    "    # Iterate through the selected year options to scrape player URLs\n",
    "    for text in first_option_texts:\n",
    "        print(f\"Processing: {text}\")\n",
    "\n",
    "        # Re-locate the dropdown each time (elements become stale after refresh)\n",
    "        dropdown_element = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.XPATH, '/html/body/app-root/div/app-rankings/div/div/div[1]/div[5]/form/div[1]/app-selector/select'))\n",
    "        )\n",
    "        dropdown = Select(dropdown_element)\n",
    "\n",
    "        # Select the dropdown option by its visible text\n",
    "        dropdown.select_by_visible_text(text)\n",
    "\n",
    "        # Ensure the selection triggers a table update\n",
    "        driver.execute_script(\"arguments[0].dispatchEvent(new Event('change'))\", dropdown_element)\n",
    "        \n",
    "        # Wait for the rankings table to update\n",
    "        time.sleep(5)\n",
    "\n",
    "        # Extract the year from the text\n",
    "        year = int(text.split()[-1][-2:])  # Extract last two digits of the year\n",
    "        max_clicks = 4 if year <= 11 else float('inf')  # Limit clicks if year is 11 or lower\n",
    "        click_count = 0\n",
    "\n",
    "        # Click the \"Show More\" button to reveal additional players, if available\n",
    "        while click_count < max_clicks:\n",
    "            try:\n",
    "                show_more_button = driver.find_elements(By.XPATH, '/html/body/app-root/div/app-rankings/div/div/div[1]/div[8]/div/button')\n",
    "                if show_more_button:\n",
    "                    driver.execute_script(\"arguments[0].click();\", show_more_button[0])\n",
    "                    time.sleep(2)  # Allow time for more results to load\n",
    "                    click_count += 1\n",
    "                else:\n",
    "                    break  # Exit loop if no button is found\n",
    "            except:\n",
    "                print('No \"Show More\" button found')\n",
    "                break\n",
    "\n",
    "        # Extract the updated table data from the webpage\n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        table = soup.find(\"table\")  # Locate the rankings table\n",
    "\n",
    "        if table:\n",
    "            rows = table.find_all(\"tr\")  # Get all table rows\n",
    "\n",
    "            # Iterate through table rows and extract player profile URLs\n",
    "            for row in rows[1:]:  # Skip the header row\n",
    "                url_suffix = row.find('a')['href']\n",
    "                player_url = 'https://matchstat.com/' + url_suffix  # Construct full URL\n",
    "\n",
    "                # Avoid duplicate URLs\n",
    "                if player_url not in player_urls:\n",
    "                    player_urls.append(player_url)\n",
    "\n",
    "        # Instead of refreshing the page entirely, reload only the relevant parts\n",
    "        driver.execute_script(\"window.location.reload();\")\n",
    "        time.sleep(5)  # Allow time for the page to reload\n",
    "    \n",
    "    # Close the WebDriver after all data has been collected\n",
    "    driver.quit()\n",
    "\n",
    "    # Print the number or URLs collected\n",
    "    print(len(player_urls), 'player urls collected')\n",
    "\n",
    "    # Save the list to a CSV file\n",
    "    pd.DataFrame(player_urls, columns=['url']).to_csv('player_stats/player_urls.csv', index=False)\n",
    "    \n",
    "    return player_urls\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then run the function and observe the outputs to ensure that we have successfully covered every year from the current year to 2001:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: 15 Dec 03\n",
      "Processing: 20 Dec 04\n",
      "Processing: 19 Dec 05\n",
      "Processing: 18 Dec 06\n",
      "Processing: 24 Dec 07\n",
      "Processing: 22 Dec 08\n",
      "Processing: 28 Dec 09\n",
      "Processing: 27 Dec 10\n",
      "Processing: 26 Dec 11\n",
      "Processing: 24 Dec 12\n",
      "Processing: 23 Dec 13\n",
      "Processing: 22 Dec 14\n",
      "Processing: 28 Dec 15\n",
      "Processing: 26 Dec 16\n",
      "Processing: 25 Dec 17\n",
      "Processing: 24 Dec 18\n",
      "Processing: 23 Dec 19\n",
      "Processing: 28 Dec 20\n",
      "Processing: 27 Dec 21\n",
      "Processing: 26 Dec 22\n",
      "Processing: 25 Dec 23\n",
      "Processing: 23 Dec 24\n",
      "Processing: 07 Apr 25\n",
      "3099 player urls collected\n"
     ]
    }
   ],
   "source": [
    "player_urls = get_player_urls('https://matchstat.com/tennis/atp-wta-rankings/atp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to webscrape all stats for all players for all years\n",
    "\n",
    "We can finally create a function that combines all of the above functions to perform the webscraping for all player stats for all years for all player urls. This function takes the player urls list as an argument, and performs all webscraping on each url adding each dictionary to a list. The function keeps a log of every 50 players scraped,  and finally saves the list of dictionaries as a DataFrame in a specified folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_and_save_player_data(player_urls, output_file='player_stats/all_player_atts.csv'):\n",
    "    \"\"\"\n",
    "    Scrapes player data for a list of player URLs and saves the results to a CSV file inside a specified folder.\n",
    "\n",
    "    Parameters:\n",
    "    - player_urls (list): List of URLs to scrape player data from.\n",
    "    - output_file (str): File path where the CSV will be saved (default: 'player_stats/all_player_atts.csv').\n",
    "\n",
    "    Returns:\n",
    "    - None: Saves the data to the specified CSV file.\n",
    "    \"\"\"\n",
    "\n",
    "    all_player_atts = []\n",
    "    total_players = len(player_urls)\n",
    "\n",
    "    for i, url in enumerate(player_urls, start=1):\n",
    "        player_atts = get_player_data(url)\n",
    "\n",
    "        # Only add valid player data to the list\n",
    "        if player_atts is not None:\n",
    "            all_player_atts.append(player_atts)\n",
    "\n",
    "        remaining = total_players - i\n",
    "        if i % 50 == 0 or remaining == 0:\n",
    "            print(f'{i} players checked, {remaining} remaining')\n",
    "\n",
    "    # Save DataFrame only if valid data exists\n",
    "    try:\n",
    "        if all_player_atts:\n",
    "            pd.DataFrame(all_player_atts).to_csv(output_file, index=False)\n",
    "            print(f\"Data successfully saved to {output_file}\")\n",
    "        else:\n",
    "            print(\"No valid player data collected. CSV file was not created.\")\n",
    "    except:\n",
    "        return all_player_atts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can load a new 'new_player_urls' file which was based on player_urls but reduced during a previous iteration of the model building where players that did not exist in the match dataset were removed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load player urls\n",
    "player_urls = pd.read_csv('player_urls/new_player_urls.csv')['url'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 players checked, 3049 remaining\n",
      "100 players checked, 2999 remaining\n",
      "150 players checked, 2949 remaining\n",
      "200 players checked, 2899 remaining\n",
      "250 players checked, 2849 remaining\n",
      "300 players checked, 2799 remaining\n",
      "350 players checked, 2749 remaining\n",
      "400 players checked, 2699 remaining\n",
      "450 players checked, 2649 remaining\n",
      "500 players checked, 2599 remaining\n",
      "550 players checked, 2549 remaining\n",
      "600 players checked, 2499 remaining\n",
      "650 players checked, 2449 remaining\n",
      "700 players checked, 2399 remaining\n",
      "750 players checked, 2349 remaining\n",
      "800 players checked, 2299 remaining\n",
      "850 players checked, 2249 remaining\n",
      "900 players checked, 2199 remaining\n",
      "Maximum retries reached for https://matchstat.com//tennis/player/Nocolas%20Alberto%20Jara%20Lozano. Exiting function.\n",
      "950 players checked, 2149 remaining\n",
      "1000 players checked, 2099 remaining\n",
      "1050 players checked, 2049 remaining\n",
      "1100 players checked, 1999 remaining\n",
      "1150 players checked, 1949 remaining\n",
      "1200 players checked, 1899 remaining\n",
      "1250 players checked, 1849 remaining\n",
      "1300 players checked, 1799 remaining\n",
      "1350 players checked, 1749 remaining\n",
      "1400 players checked, 1699 remaining\n",
      "1450 players checked, 1649 remaining\n",
      "1500 players checked, 1599 remaining\n",
      "1550 players checked, 1549 remaining\n",
      "1600 players checked, 1499 remaining\n",
      "1650 players checked, 1449 remaining\n",
      "1700 players checked, 1399 remaining\n",
      "1750 players checked, 1349 remaining\n",
      "1800 players checked, 1299 remaining\n",
      "1850 players checked, 1249 remaining\n",
      "1900 players checked, 1199 remaining\n",
      "1950 players checked, 1149 remaining\n",
      "2000 players checked, 1099 remaining\n",
      "2050 players checked, 1049 remaining\n",
      "2100 players checked, 999 remaining\n",
      "2150 players checked, 949 remaining\n",
      "2200 players checked, 899 remaining\n",
      "2250 players checked, 849 remaining\n",
      "2300 players checked, 799 remaining\n",
      "2350 players checked, 749 remaining\n",
      "2400 players checked, 699 remaining\n",
      "2450 players checked, 649 remaining\n",
      "2500 players checked, 599 remaining\n",
      "2550 players checked, 549 remaining\n",
      "2600 players checked, 499 remaining\n",
      "2650 players checked, 449 remaining\n",
      "2700 players checked, 399 remaining\n",
      "2750 players checked, 349 remaining\n",
      "2800 players checked, 299 remaining\n",
      "2850 players checked, 249 remaining\n",
      "2900 players checked, 199 remaining\n",
      "2950 players checked, 149 remaining\n",
      "3000 players checked, 99 remaining\n",
      "3050 players checked, 49 remaining\n",
      "3099 players checked, 0 remaining\n",
      "Data successfully saved to player_stats/all_player_atts.csv\n"
     ]
    }
   ],
   "source": [
    "# Run scraping function\n",
    "scrape_and_save_player_data(player_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now successfully created our player dataset which contains multiple stats for each player across multiple years. This should give us a lot of data to work with to engineer features when building our betting model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
